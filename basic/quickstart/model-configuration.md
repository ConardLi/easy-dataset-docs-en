---
icon: gear-complex-code
---

# Model Configuration

{% hint style="info" %}
&#x20;This module is used to configure the large models needed for subsequent literature processing, dataset construction, and other functions, including text models and vision models.
{% endhint %}

<figure><img src="../../.gitbook/assets/image (53).png" alt=""><figcaption></figcaption></figure>

Currently, the platform has built-in some model providers by default. You only need to fill in the corresponding key for the model provider:

<table><thead><tr><th width="139.87890625">ProviderId</th><th width="136.83203125">Name</th><th>API URL</th></tr></thead><tbody><tr><td>ollama</td><td>Ollama</td><td>http://127.0.0.1:11434/api</td></tr><tr><td>openai</td><td>OpenAI</td><td>https://api.openai.com/v1/</td></tr><tr><td>siliconcloud</td><td>Silicon Flow</td><td>https://api.ap.siliconflow.com/v1/</td></tr><tr><td>deepseek</td><td>DeepSeek</td><td>https://api.deepseek.com/v1/</td></tr><tr><td>302ai</td><td>302.AI</td><td>https://api.302.ai/v1/</td></tr><tr><td>zhipu</td><td>Zhipu AI</td><td>https://open.bigmodel.cn/api/paas/v4/</td></tr><tr><td>Doubao</td><td>Volcano Engine</td><td>https://ark.cn-beijing.volces.com/api/v3/</td></tr><tr><td>groq</td><td>Groq</td><td>https://api.groq.com/openai</td></tr><tr><td>grok</td><td>Grok</td><td>https://api.x.ai</td></tr><tr><td>openRouter</td><td>OpenRouter</td><td>https://openrouter.ai/api/v1/</td></tr><tr><td>alibailian</td><td>Alibaba Cloud Bailian</td><td>https://dashscope.aliyuncs.com/compatible-mode/v1</td></tr></tbody></table>

{% hint style="success" %}
Note: Model providers not in the above list are also supported for configuration. Information such as model provider, API interface address, API Key, and model name all support custom input. As long as the API conforms to the OPEN AI format, the platform can be compatible with it.
{% endhint %}

<figure><img src="../../.gitbook/assets/image (54).png" alt=""><figcaption></figcaption></figure>

Click **Refresh Model List** to view all models provided by the provider (you can also manually enter the model name here):

<figure><img src="../../.gitbook/assets/image (55).png" alt=""><figcaption></figcaption></figure>

Supports configuration of language models (for text generation tasks) and vision models (for visual analysis tasks):

<figure><img src="../../.gitbook/assets/image (66).png" alt=""><figcaption></figcaption></figure>

It also supports configuring the model's temperature and maximum output tokens:

<figure><img src="../../.gitbook/assets/image (52).png" alt=""><figcaption></figcaption></figure>

* **Temperature**: Controls the randomness of the generated text. Higher temperature results in more random and diverse outputs, while lower temperature leads to more stable and conservative outputs.
* **Max Token**: Limits the length of text generated by the model, measured in tokens, to prevent excessively long outputs.

***

Supports Ollama, which can automatically fetch the list of locally deployed models:

<figure><img src="../../.gitbook/assets/image (47).png" alt=""><figcaption></figcaption></figure>

Supports configuring multiple models, which can be switched through the model dropdown box in the upper right corner:

<figure><img src="../../.gitbook/assets/image (48).png" alt=""><figcaption></figcaption></figure>
